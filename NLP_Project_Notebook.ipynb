{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad28a52",
   "metadata": {},
   "source": [
    "# NLP Project: Word2Vec and Embeddings Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894fa04",
   "metadata": {},
   "source": [
    "This notebook implements Word2Vec embeddings for a dataset of questions, visualizes the embeddings in both 2D and 3D, and calculates similarities between questions using cosine similarity. It also demonstrates how to retrieve similar questions for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d60af1",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The dataset consists of Stack Overflow questions with titles, tags, and question bodies. We preprocess the text data by cleaning and tokenizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')[['Title', 'Tags', 'Body']]\n",
    "valid_data = pd.read_csv('valid.csv')[['Title', 'Tags', 'Body']]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "train_data['Title'] = train_data['Title'].apply(preprocess_text)\n",
    "train_data['Body'] = train_data['Body'].apply(preprocess_text)\n",
    "valid_data['Title'] = valid_data['Title'].apply(preprocess_text)\n",
    "valid_data['Body'] = valid_data['Body'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32252a1",
   "metadata": {},
   "source": [
    "## Training Word2Vec Model\n",
    "We use the Gensim library to train a Word2Vec model on the titles and bodies of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_data['Title'].tolist() + train_data['Body'].tolist()\n",
    "sentences = [sentence.split() for sentence in sentences if isinstance(sentence, str)]\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "model.save('word2vec_model.model')  # Save the model for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd9e4a",
   "metadata": {},
   "source": [
    "## Embedding Visualization\n",
    "We visualize the embeddings using PCA for 3D and t-SNE/UMAP for 2D visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "words = list(word_vectors.index_to_key)[:100]\n",
    "embeddings = np.array([word_vectors[word] for word in words])\n",
    "\n",
    "# PCA for 3D visualization\n",
    "pca = PCA(n_components=3)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot 3D visualization\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2])\n",
    "for i, word in enumerate(words):\n",
    "    ax.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], reduced_embeddings[i, 2], word)\n",
    "plt.title('Word Embeddings (3D)')\n",
    "plt.show()\n",
    "\n",
    "# t-SNE for 2D visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "reduced_embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(reduced_embeddings_2d[:, 0], reduced_embeddings_2d[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.text(reduced_embeddings_2d[i, 0], reduced_embeddings_2d[i, 1], word)\n",
    "plt.title('Word Embeddings (2D)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af6137",
   "metadata": {},
   "source": [
    "## Finding Similar Questions\n",
    "We compute document vectors by averaging the word vectors of each question and use cosine similarity to find the most similar questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    words = [word for word in doc.split() if word in model.wv.index_to_key]\n",
    "    return np.mean([model.wv[word] for word in words], axis=0) if words else np.zeros(model.vector_size)\n",
    "\n",
    "query = 'How to train a machine learning model?'\n",
    "query_vector = document_vector(query)\n",
    "\n",
    "train_data['DocVector'] = train_data['Title'] + ' ' + train_data['Body']\n",
    "train_data['DocVector'] = train_data['DocVector'].apply(document_vector)\n",
    "\n",
    "similarities = train_data['DocVector'].apply(lambda x: cosine_similarity([query_vector], [x])[0][0])\n",
    "top_matches = train_data.iloc[similarities.nlargest(5).index]\n",
    "print('Query:', query)\n",
    "print('Top 5 Similar Questions:')\n",
    "for i, row in top_matches.iterrows():\n",
    "    print(f\"{i+1}. {row['Title']} (Similarity: {similarities[i]:.2f})\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
